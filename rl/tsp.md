å¯ä»¥ç›´æ¥æŠ„ https://ekimetrics.github.io/blog/2021/11/03/tsp/   2021å¹´çš„è¿™ç¯‡æ–‡ç« 







ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æœ€åŸºç¡€çš„ç®—æ³•ä½†æ˜¯è¦ä½¿ç”¨mlpå¤šå±‚ç¥ç»ç½‘ç»œï¼Œä¸ä½¿ç”¨æŒ‡é’ˆç­‰ç¥ç»ç½‘ç»œï¼Œæ¥è§£å†³tspé—®é¢˜ã€‚è®¾è®¡æ–¹æ¡ˆéœ€è¦è€ƒè™‘å¦‚ä¸‹å†…å®¹ï¼šèƒ½å¦ç›´æ¥è¾“å…¥one hotç¼–ç +å…¶ä»–è¾“å…¥ï¼ˆå…¶ä¸­0ä¸ºæœªè®¿é—®çš„åŸå¸‚ï¼Œ1ä¸ºè®¿é—®çš„åŸå¸‚ï¼Œä»¥è®¿é—® åŸå¸‚çš„è·ç¦»ä¸ºè´Ÿæˆ–è€…å€’æ•°å®šä¹‰ä¸ºrewardï¼Œæˆ–è€…å…¶ä»–æ›´å¥½çš„æ–¹æ¡ˆï¼‰ï¼›é—®é¢˜æ˜¯10-50ä¸ªåŸå¸‚æ•°é‡ï¼›tspçš„è·¯å¾„è¦æ±‚å›åˆ°èµ·ç‚¹ï¼›çŠ¶æ€ä¹Ÿéœ€è¦ä½ è®¾è®¡ï¼›ä¸é™åˆ¶ä½¿ç”¨æ¨¡å‹ï¼Œå¯ä»¥ç”¨dqnï¼Œsarsaã€actor-criticéƒ½å¯ä»¥ã€‚æœ€åçš„æ•ˆæœè¦æ±‚å¥½ï¼Œä¹Ÿè¦æ±‚ç»™å‡ºè®¾è®¡æ–¹æ¡ˆçš„åˆ›æ–°ç‚¹å’Œå¯¹åº”è§£å†³é—®é¢˜ã€‚





å°†åŸºäºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„åŸºæœ¬ç®—æ³•ï¼Œä½¿ç”¨å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ç¥ç»ç½‘ç»œæ¥è§£å†³æ—…è¡Œå•†é—®é¢˜ï¼ˆTSPï¼‰ã€‚æˆ‘ä¸ä¼šä½¿ç”¨æŒ‡é’ˆç½‘ç»œï¼ˆPointer Networkï¼‰æˆ–å…¶ä»–é«˜çº§ç¥ç»ç½‘ç»œç»“æ„ï¼ˆå¦‚Transformerï¼‰ï¼Œè€Œæ˜¯åšæŒä½¿ç”¨ç®€å•çš„MLPä½œä¸ºç­–ç•¥æˆ–ä»·å€¼å‡½æ•°çš„è¿‘ä¼¼å™¨ã€‚æ–¹æ¡ˆè®¾è®¡è€ƒè™‘äº†æ‚¨æåˆ°çš„ç‚¹ï¼šone-hotç¼–ç  + å…¶ä»–è¾“å…¥ã€rewardå®šä¹‰ï¼ˆåŒ…æ‹¬æ‚¨å»ºè®®çš„è´Ÿè·ç¦»æˆ–å€’æ•°ï¼Œä»¥åŠæˆ‘æå‡ºçš„æ›´å¥½æ–¹æ¡ˆï¼‰ã€10-50ä¸ªåŸå¸‚è§„æ¨¡ã€è·¯å¾„å¿…é¡»è¿”å›èµ·ç‚¹ã€çŠ¶æ€è®¾è®¡ã€æ¨¡å‹é€‰æ‹©ï¼ˆæˆ‘é€‰æ‹©DQNä½œä¸ºåŸºç¡€ç®—æ³•ï¼Œä½†ä¹Ÿè®¨è®ºäº†SARSAå’ŒActor-Criticçš„å˜ä½“ï¼‰ã€‚æ–¹æ¡ˆæ—¨åœ¨å®ç°è¾ƒå¥½çš„æ•ˆæœï¼ˆé’ˆå¯¹å°è§„æ¨¡TSPï¼ŒRLç»“åˆMLPå¯ä»¥é€¼è¿‘æœ€ä¼˜è§£ï¼Œå°¤å…¶åœ¨10-20åŸå¸‚æ—¶ï¼›å¯¹äº50åŸå¸‚ï¼Œå¯èƒ½éœ€è¦æ›´å¤šè®­ç»ƒè¿­ä»£ï¼Œä½†é€šè¿‡reward shapingå¯ä»¥æå‡æ”¶æ•›ï¼‰ã€‚







ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æœ€åŸºç¡€çš„ç®—æ³•ï¼Œä¸ä½¿ç”¨æŒ‡é’ˆç­‰ç¥ç»ç½‘ç»œï¼Œæ¥è§£å†³tspé—®é¢˜ï¼Œå¦‚ä½•è§£å†³tspé—®é¢˜ï¼Œèƒ½å¦ç›´æ¥è¾“å…¥one hotç¼–ç +å…¶ä»–è¾“å…¥ï¼ˆå…¶ä¸­0ä¸ºæœªè®¿é—®çš„åŸå¸‚ï¼Œ1ä¸ºè®¿é—®çš„åŸå¸‚ï¼Œä»¥è®¿é—® åŸå¸‚çš„è·ç¦»ä¸ºè´Ÿå®šä¹‰ä¸ºrewardè¿›è¡Œä¼˜åŒ–ï¼‰ã€‚è¦æ±‚è®¾è®¡çš„æ–¹æ¡ˆæ•ˆæœå¥½ï¼Œä¸è¦ä½¿ç”¨q learningçš„æ–¹å¼ï¼Œå› ä¸ºtableå¤ªå°ã€‚å¹¶ä¸”ä¸é™åˆ¶ä½¿ç”¨çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•å¦‚dqnã€sarsaã€actor-criticç­‰ï¼Œå¹¶ä¸”åˆ—å‡ºåˆ›æ–°ç‚¹å’Œè§£å†³çš„é—®é¢˜





**ç©ºé—´ç»“æ„ï¼ˆåŸå¸‚è·ç¦»ï¼‰å¹¶æ²¡æœ‰ç›´æ¥ç¼–ç åˆ° state ä¸­**ã€‚

é‚£ä¹ˆ Q å€¼çš„æ›´æ–°åªèƒ½â€œä» reward ä¸­â€é—´æ¥æ¨æ–­å‡ºè·ç¦»ç»“æ„ï¼šæ¯”å¦‚ï¼šå¦‚æœä» A â†’ B ç»™äº† -2 rewardï¼Œç³»ç»Ÿé€æ¸å­¦åˆ° A â†’ B ä¸å¦‚ A â†’ Cï¼ˆreward=-1ï¼‰ã€‚

è¿™ç§æ–¹å¼æ˜¯â€œé»‘ç›’è®°å¿†å¼â€çš„ï¼Œ**æ•ˆç‡è¿œä¸å¦‚ç›´æ¥è¾“å…¥åŸå¸‚åæ ‡æˆ–è·ç¦»çŸ©é˜µ**ã€‚



æ¯ä¸€æ­¥ç§»åŠ¨éƒ½æœ‰ rewardï¼ˆè´Ÿè·ç¦»ï¼‰ï¼Œ**ä¸æ˜¯å…¸å‹çš„ç¨€ç–å¥–åŠ±é—®é¢˜**ï¼ˆä¾‹å¦‚åªæœ‰ç»ˆç‚¹ç»™å¥–åŠ±ï¼‰ã€‚

æ‰€ä»¥**reward æ˜¯ç¨ å¯†çš„**ï¼Œæ¯ä¸€æ­¥éƒ½æœ‰ä¿¡æ¯ã€‚





**æ•æ‰ç©ºé—´å…³ç³»**ï¼šæ¨¡å‹éœ€ç†è§£åŸå¸‚åæ ‡ (x,y)(x, y)(x,y) ä¹‹é—´çš„å‡ ä½•å…³ç³»ï¼ˆå¦‚æ¬§å‡ é‡Œå¾—è·ç¦»ï¼‰ï¼Œä»¥ç”Ÿæˆæ›´ä¼˜çš„å·¡æ¸¸è·¯å¾„ã€‚

**æ•æ‰åºåˆ—ä¾èµ–**ï¼šæ¨¡å‹éœ€å»ºæ¨¡è®¿é—®é¡ºåºçš„ä¾èµ–æ€§ï¼Œç¡®ä¿ç”Ÿæˆåˆæ³•çš„ TSP è§£ï¼ˆæ¯ä¸ªåŸå¸‚è®¿é—®ä¸€æ¬¡å¹¶è¿”å›èµ·ç‚¹ï¼‰ã€‚







- - **ç©ºé—´å…³ç³»**ï¼šQ-Learning é€šè¿‡å¥–åŠ±é—´æ¥å­¦ä¹ è·ç¦»å…³ç³»ï¼Œç¼ºä¹æŒ‡é’ˆç½‘ç»œçš„ç»“æ„åŒ–å»ºæ¨¡ï¼ˆå¦‚æ³¨æ„åŠ›æœºåˆ¶æ•æ‰å…¨å±€å‡ ä½•ï¼‰ã€‚

  - **åºåˆ—ä¾èµ–**ï¼šQ-Learning çš„çŠ¶æ€ä»…è®°å½•è®¿é—®çŠ¶æ€å’Œå½“å‰åŸå¸‚ï¼Œåºåˆ—ä¾èµ–å»ºæ¨¡è¾ƒå¼±ï¼ŒæŒ‡é’ˆç½‘ç»œé€šè¿‡ LSTM/æ³¨æ„åŠ›æ•æ‰é•¿æœŸä¾èµ–ã€‚

  - **çŠ¶æ€ç©ºé—´çˆ†ç‚¸**ï¼šTSP50 çš„çŠ¶æ€ç©ºé—´ä¸º 250â‹…50 2^{50} \cdot 50 250â‹…50ï¼ŒQ è¡¨å­˜å‚¨å’Œæ”¶æ•›å›°éš¾ï¼ŒæŒ‡é’ˆç½‘ç»œé€šè¿‡ç¥ç»ç½‘ç»œæ³›åŒ–ã€‚

  - ä½¿ç”¨åŸºç¡€çš„ Q-Learning ç®—æ³•ï¼Œé€šè¿‡ one-hot ç¼–ç ï¼ˆ0 è¡¨ç¤ºæœªè®¿é—®ï¼Œ1 è¡¨ç¤ºå·²è®¿é—®ï¼‰å’Œè´Ÿè·ç¦»å¥–åŠ±ï¼Œå¯ä»¥è§£å†³ TSPï¼Œç”Ÿæˆåˆæ³•æ’åˆ—ã€‚è®¾è®¡ä¸Šé€š

  - ï¼šTSP çš„æœ€ä¼˜è§£ä¾èµ–äºæ•´ä¸ªè·¯å¾„çš„é€‰æ‹©ã€‚ä¾‹å¦‚ï¼Œé€‰æ‹©æŸä¸ªåŸå¸‚ä½œä¸ºæ—©æœŸè®¿é—®ç‚¹å¯èƒ½å½±å“åç»­è·¯å¾„çš„æ€»é•¿åº¦ï¼ˆä¾‹å¦‚ï¼Œç»•è¿‡è¿œè·ç¦»åŸå¸‚ï¼‰ã€‚

    **ç´¯è®¡æ•ˆåº”**ï¼šæ—©æœŸåŠ¨ä½œï¼ˆé€‰æ‹©åŸå¸‚ï¼‰å¯¹åç»­çŠ¶æ€å’Œå¥–åŠ±æœ‰é•¿æœŸå½±å“ã€‚ä¾‹å¦‚ï¼Œæ—©æœŸé€‰æ‹©è¾ƒè¿œçš„åŸå¸‚å¯èƒ½å¯¼è‡´åç»­è·¯å¾„è¢«è¿«ç»•è¡Œï¼Œå¢åŠ æ€»é•¿åº¦ã€‚

    **Q-Learning çš„å±€é™**ï¼šQ-Learning ä½¿ç”¨ one-hot ç¼–ç å’Œå½“å‰åŸå¸‚è¡¨ç¤ºçŠ¶æ€ï¼Œåºåˆ—ä¾èµ–ä»…é€šè¿‡å½“å‰çŠ¶æ€é—´æ¥å»ºæ¨¡ï¼Œéš¾ä»¥æ•æ‰æ—©æœŸå†³ç­–å¯¹æ•´ä¸ªè·¯å¾„çš„å½±å“ã€‚

    **å¯¹æ¯”ç¥ç»ç½‘ç»œ**ï¼šBello et al. (2016) çš„æŒ‡é’ˆç½‘ç»œé€šè¿‡ LSTM å’Œæ³¨æ„åŠ›æœºåˆ¶æ•æ‰é•¿æœŸä¾èµ–ï¼ˆä¾‹å¦‚ï¼ŒDecoder è®°ä½å·²è®¿é—®åºåˆ—çš„å†å²ï¼‰ï¼Œç”Ÿæˆæ›´ä¼˜è·¯å¾„ï¼ˆTSP50 å¹³å‡é•¿åº¦ 6.09ï¼Œæ¥è¿‘æœ€ä¼˜ 5.69ï¼‰ã€‚

  - **å†å²è·¯å¾„ä¿¡æ¯**ï¼šåœ¨çŠ¶æ€ä¸­åŠ å…¥å·²è®¿é—®åŸå¸‚çš„é¡ºåºä¿¡æ¯ï¼Œä¾‹å¦‚ï¼š

    - è®°å½•æœ€è¿‘ k k k ä¸ªè®¿é—®åŸå¸‚çš„ç´¢å¼•ï¼šht=[Ï€tâˆ’k,â€¦,Ï€tâˆ’1] h_t = [\pi_{t-k}, \ldots, \pi_{t-1}] ht=[Ï€tâˆ’k,â€¦,Ï€tâˆ’1]ã€‚
    - æˆ–å°†å·²è®¿é—®åŸå¸‚çš„è·ç¦»å‡å€¼åŠ å…¥çŠ¶æ€ï¼šmean([DÏ€i,Ï€i+1âˆ£iâˆˆvisited]) \text{mean}([D_{\pi_i, \pi_{i+1}} \mid i \in \text{visited}]) mean([DÏ€i,Ï€i+1âˆ£iâˆˆvisited])ã€‚

    **åˆ†å±‚ç­–ç•¥**ï¼šå°† TSP åˆ†è§£ä¸ºå­åŒºåŸŸï¼Œåˆ†åˆ«ä¼˜åŒ–å­è·¯å¾„ï¼Œå‡å°‘é•¿æœŸä¾èµ–çš„å¤æ‚æ€§ã€‚

    **é—®é¢˜æœ¬è´¨**ï¼šè™½ç„¶ TSP çš„ç›®æ ‡æ˜¯è®¿é—®é¡ºåºï¼Œä½†ä¼˜åŒ–é«˜è´¨é‡é¡ºåºéœ€è¦è€ƒè™‘å…¨å±€è·¯å¾„ç»“æ„ï¼Œé•¿æœŸä¾èµ–å½±å“è·¯å¾„çš„æ•´ä½“æ•ˆç‡



TSP æ˜¯ä¸€ä¸ª NP éš¾çš„çµ„åˆä¼˜åŒ–é—®é¢˜ï¼Œè§£ç©ºé—´å¤§å°ä¸º n! n! n!ï¼ˆn ä¸ªåŸå¸‚çš„æ’åˆ—æ•°ï¼‰ã€‚ç›´æ¥ä½¿ç”¨ç­–ç•¥æ¢¯åº¦ä¼˜åŒ–ï¼ˆä¸ç»“åˆç»“æ„åŒ–æ¨¡å‹ï¼‰éœ€è¦å®šä¹‰ä¸€ä¸ªå›ºå®šçš„åŠ¨ä½œç©ºé—´ï¼Œéš¾ä»¥å¤„ç†ï¼š

- **å˜é•¿åŠ¨ä½œç©ºé—´**ï¼šåŸå¸‚æ•°é‡ n n n å¯å˜ï¼ŒåŠ¨ä½œç©ºé—´ï¼ˆé€‰æ‹©ä¸‹ä¸€ä¸ªåŸå¸‚ï¼‰éšå®ä¾‹å˜åŒ–ã€‚



é—®é¢˜ï¼š

## TSP ä¸€å®šè¦å»ºæ¨¡åºåˆ—ä¾èµ–ï¼Œä¸å»ºæ¨¡é¡ºåºå°±ç­‰äºä¸èƒ½è§£å†³é—®é¢˜

> TSP reward æ˜¯**åºåˆ—ç»“æ„ä¾èµ–å‹**ï¼Œè€Œè¿·å®« reward æ˜¯**ç›®æ ‡çŠ¶æ€ä¾èµ–å‹**



**è´Ÿ reward å…¨å±€åå° â†’ Q å€¼ä¼šå˜å¾—å¾ˆè´Ÿï¼Œæ”¶æ•›æ…¢ï¼š**

- Q æ›´æ–°æ­¥æ­¥éƒ½æ˜¯è´Ÿçš„ï¼Œå®¹æ˜“å¯¼è‡´ Q å€¼ä¸‹é™åˆ°æä½ï¼Œåå‘ä¼ æ’­æ•ˆæœå˜å·®ï¼ˆæ¢¯åº¦å¾®å¼±ï¼‰ã€‚

**ç­–ç•¥æ”¶æ•›æ…¢çš„æ ¹å› ï¼šç»„åˆç©ºé—´å¤§**

- å³ä½¿ reward ä¸ç¨€ç–ï¼Œä½†çŠ¶æ€ç©ºé—´æ˜¯ `O(N Ã— 2^N)`ï¼Œå¯¼è‡´è¦æ¢ç´¢å¤§é‡ episodeã€‚
- ç‰¹åˆ«åœ¨è´ªå¿ƒç­–ç•¥æ”¶æ•›å‰ï¼Œå®¹æ˜“è¢«æ¬¡ä¼˜è·¯å¾„å›°ä½ã€‚

æ–¹æ¡ˆï¼š

ä½¿ç”¨ reward normalization æˆ– reward scalingï¼ˆå¦‚é™¤ä»¥æœ€å¤§è·ç¦»ï¼Œæˆ–ä½¿ç”¨ softmax Q æ›´æ–°ï¼‰ã€‚

æ·»åŠ å›åˆ°èµ·ç‚¹çš„é¢å¤–å¥–åŠ±ï¼Œé¼“åŠ±å®Œæˆå›è·¯ã€‚





å°±åƒä¸ç»™ä½ åœ°å›¾ï¼Œåªè®©ä½ èµ°ï¼Œèµ°å¤šäº†æ‰çŸ¥é“å“ªæ¡è·¯è¿‘ã€‚Q-learning æ˜¯å‡­ç»éªŒç§¯ç´¯åœ°å›¾ï¼Œè€Œä¸æ˜¯çœ‹åœ°å›¾åšå†³ç­–ã€‚

- âœ… å¯ä»¥é—´æ¥å­¦åˆ°åŸå¸‚é—´å…³ç³»ï¼ˆé€šè¿‡ reward é©±åŠ¨ï¼‰ã€‚
- âŒ ä½†æ•ˆç‡ä½ã€ä¸æ³›åŒ–ã€ä¸ç¨³å¥ã€‚
- è‹¥æƒ³åŠ å¿«å­¦ä¹ æ•ˆæœï¼Œå¯è€ƒè™‘ï¼šå°†åŸå¸‚å¯¹ä¹‹é—´çš„è·ç¦»ä½œä¸ºä¸€éƒ¨åˆ† state æˆ–ç‰¹å¾è¾…åŠ©å­¦ä¹ ï¼ˆå³ state = å½“å‰åŸå¸‚ + visited çŠ¶æ€ + åˆ°æ‰€æœ‰åŸå¸‚çš„è·ç¦»å‘é‡ï¼‰ã€‚





åˆ†å±‚ç­–ç•¥

**reward shaping** refers to the technique of modifying or augmenting the reward function in reinforcement learning (RL) to guide the agent's learning process more effectively. The goal is to provide additional intermediate rewards or penalties that help the agent learn desired behaviors faster or avoid poor strategies, without altering the optimal policy of the original task. It is often used to address sparse reward problems (where meaningful rewards are rare) by adding denser, domain-specific feedback signals



**Dense reward** refers to a reward function in reinforcement learning that provides frequent, fine-grained feedback to the agent at each step or state transition. Unlike sparse rewards (where feedback is given only upon task completion or rare milestones), dense rewards guide the agent more continuously by shaping behavior incrementally. This can accelerate learning but requires careful design to avoid unintended biases or suboptimal policies.





**State aliasing** **çŠ¶æ€æ··æ·†**occurs in reinforcement learning (RL) when two or more distinct environmental states are mapped to the same internal representation by the agent's perception system (e.g., due to limited sensors or function approximation). This can lead to the agent treating different states as identical, causing suboptimal or even dangerous decisions. State aliasing is a key challenge in **partial observability** and can contribute to perceptual aliasing (where different observations appear the same).









## ä¸€ã€å¯¹æ¯”æŒ‡æ ‡è®¾è®¡

ç›®æ ‡æ˜¯å›´ç»•ä¸¤å¤§ç ”ç©¶ç»´åº¦ï¼š

- **ä¸åŒçŠ¶æ€è¡¨ç¤ºçš„ç›¸å¯¹è´¡çŒ®**ï¼ˆè´¡çŒ®åº¦åˆ†æï¼‰
- **æ³›åŒ–æ€§åˆ†æ**ï¼ˆzero-shotèƒ½åŠ›ï¼‰

æˆ‘ä»¬å°†ç»¼åˆä½¿ç”¨ä»¥ä¸‹æŒ‡æ ‡è¿›è¡Œå¯¹æ¯”ï¼š

### 1. æ”¶æ•›æ€§æŒ‡æ ‡

| æŒ‡æ ‡                                      | å®šä¹‰                                    | ç”¨é€”           |
| ----------------------------------------- | --------------------------------------- | -------------- |
| **æ”¶æ•›é€Ÿåº¦ (Convergence Speed)**          | è¾¾åˆ° `95% æœ€ä¼˜è·¯å¾„é•¿åº¦` æ‰€éœ€ episode æ•° | è¡¡é‡å­¦ä¹ æ•ˆç‡   |
| **æ”¶æ•›æˆåŠŸç‡ (Convergence Success Rate)** | è¾¾åˆ°æ”¶æ•›æ¡ä»¶çš„å®éªŒæ¯”ä¾‹                  | è¡¡é‡ç®—æ³•é²æ£’æ€§ |
| **æ”¶æ•›episodeæ ‡å‡†å·®**                     | ä¸åŒè¿è¡Œä¹‹é—´çš„ episode æ•°æ³¢åŠ¨           | è¡¡é‡ç¨³å®šæ€§     |



### 2. æ€§èƒ½æŒ‡æ ‡

| æŒ‡æ ‡                           | å®šä¹‰                                                         | ç”¨é€”                     |
| ------------------------------ | ------------------------------------------------------------ | ------------------------ |
| **æœ€ç»ˆè·¯å¾„é•¿åº¦å‡å€¼**           | æœ€å100 episode çš„å¹³å‡è·¯å¾„é•¿åº¦                               | è¡¡é‡æœ€ç»ˆè§£è´¨é‡           |
| **æœ€ä¼˜æ€§Gap (Optimality Gap)** | (path_lengthâˆ’optimal)/optimalÃ—100%(path\_length - optimal) / optimal Ã— 100\%(path_lengthâˆ’optimal)/optimalÃ—100% | è¡¡é‡è§£çš„æ¥è¿‘ç¨‹åº¦         |
| **è·¯å¾„æ•ˆç‡å¥–åŠ±å‡å€¼**           | `path_efficiency_reward = optimal / path_length`             | æ›¿ä»£è·¯å¾„é•¿åº¦ï¼Œç»Ÿä¸€å½’ä¸€åŒ– |



### 3. ç¨³å®šæ€§ä¸é²æ£’æ€§

| æŒ‡æ ‡                      | å®šä¹‰                               | ç”¨é€”                 |
| ------------------------- | ---------------------------------- | -------------------- |
| **å˜å¼‚ç³»æ•° (CV)**         | `std / mean`ï¼Œå¯¹è·¯å¾„é•¿åº¦ç­‰æŒ‡æ ‡è®¡ç®— | è¡¡é‡ä¸åŒè¿è¡Œçš„ç¨³å®šæ€§ |
| **å¤±è´¥ç‡ (Failure Rate)** | `1 - æœ‰æ•ˆè§£æ•°é‡ / æ€»å®éªŒæ•°`        | è¡¡é‡æç«¯å¤±è´¥é£é™©     |
| **æœ€å·®æ€§èƒ½ (Worst-Case)** | æ‰€æœ‰å®éªŒä¸­æœ€å·®è·¯å¾„é•¿åº¦             | è¡¡é‡ä¸‹ç•Œé£é™©æ§åˆ¶     |



### 4. æ³›åŒ–æ€§æŒ‡æ ‡

| æŒ‡æ ‡                                 | å®šä¹‰                                                         | ç”¨é€”                   |
| ------------------------------------ | ------------------------------------------------------------ | ---------------------- |
| **Generalization Gap**               | test optimality gapâˆ’train optimality gap\text{test optimality gap} - \text{train optimality gap}test optimality gapâˆ’train optimality gap | è¡¡é‡train-testæ³›åŒ–èƒ½åŠ› |
| **Cross-Instance Test Success Rate** | `æœ‰æ•ˆè§£æ¯”ä¾‹ï¼ˆcross_instance + testï¼‰`                        | è¡¡é‡zero-shotæ³›åŒ–èƒ½åŠ›  |



------

## äºŒã€æŒ‡æ ‡è®¡ç®—å…¬å¼

å…¬å¼å¦‚ä¸‹ï¼š

```
text


å¤åˆ¶ç¼–è¾‘
1. optimality_gap = (L_path - L_opt) / L_opt * 100
2. path_efficiency_reward = L_opt / L_path
3. convergence_speed = min{episode | path_efficiency_reward >= 0.95}
4. convergence_success_rate = count(converged) / total_runs
5. stability_cv = std(L_path) / mean(L_path)
6. failure_rate = count(valid_solution == False) / total_runs
7. generalization_gap = test_optimality_gap - train_optimality_gap
```

------

## ä¸‰ã€å¯è§†åŒ–æ–¹æ¡ˆè®¾è®¡

### ã€ç ”ç©¶ç›®æ ‡1ã€‘çŠ¶æ€è¡¨ç¤ºçš„ç›¸å¯¹è´¡çŒ®åˆ†æï¼ˆåŸºäº per-instance æ¨¡å¼ï¼‰

#### ğŸ”¹å›¾1ï¼šæ”¶æ•›é€Ÿåº¦å¯¹æ¯”å›¾

- ç±»å‹ï¼šç®±çº¿å›¾
- xè½´ï¼šçŠ¶æ€è¡¨ç¤ºï¼ˆstate_typeï¼‰
- yè½´ï¼šæ”¶æ•›episodeæ•°ï¼ˆconvergence_speedï¼‰
- åˆ†ç»„ï¼šç®—æ³•ç±»å‹ + åŸå¸‚æ•°é‡

#### ğŸ”¹å›¾2ï¼šæœ€ç»ˆæ€§èƒ½çƒ­åŠ›å›¾

- ç±»å‹ï¼šheatmap
- åæ ‡ï¼šx = çŠ¶æ€è¡¨ç¤ºï¼Œy = ç®—æ³•
- æ•°å€¼ï¼šæœ€ç»ˆè·¯å¾„é•¿åº¦å‡å€¼ / æœ€ä¼˜è·¯å¾„é•¿åº¦

#### ğŸ”¹å›¾3ï¼šè´¡çŒ®é›·è¾¾å›¾ï¼ˆå„æŒ‡æ ‡ç»¼åˆï¼‰

- ç±»å‹ï¼šé›·è¾¾å›¾
- æŒ‡æ ‡ï¼šæ”¶æ•›é€Ÿåº¦ã€Gapã€å¤±è´¥ç‡ã€CVã€æœ€ä¼˜å¥–åŠ±
- è¯´æ˜ï¼šå±•ç¤ºâ€œfullâ€ä¸â€œablation_xâ€çš„ç›¸å¯¹å·®è·

------

### ã€ç ”ç©¶ç›®æ ‡2ã€‘æ³›åŒ–æ€§åˆ†æï¼ˆåŸºäº cross-instance æ¨¡å¼ï¼‰

#### ğŸ”¹å›¾4ï¼štrain/test optimality gap å¯¹æ¯”æŸ±çŠ¶å›¾

- ç±»å‹ï¼šåˆ†ç»„æŸ±çŠ¶å›¾
- xè½´ï¼šstate_type
- yè½´ï¼šoptimality gap
- hueï¼štrain/test

#### ğŸ”¹å›¾5ï¼šGeneralization Gap åˆ†å¸ƒ

- ç±»å‹ï¼šå°æç´å›¾ / ç®±çº¿å›¾
- yè½´ï¼šgeneralization_gap
- xè½´ï¼šstate_type
- åˆ†ç»„ï¼šåŸå¸‚æ•° + ç®—æ³•

#### ğŸ”¹å›¾6ï¼šzero-shotæˆåŠŸç‡è¶‹åŠ¿çº¿

- ç±»å‹ï¼šæŠ˜çº¿å›¾
- xè½´ï¼šåŸå¸‚æ•°é‡
- yè½´ï¼štestæˆåŠŸç‡
- çº¿ï¼šä¸åŒçŠ¶æ€è¡¨ç¤ºï¼ˆstate_typeï¼‰

------

## å››ã€æ¨èåˆ†æç­–ç•¥ï¼ˆåšå£«çº§ï¼‰

1. **æ˜¾è‘—æ€§æ£€éªŒ**ï¼š
   - æ¯ä¸ªstate_typeä¹‹é—´ä½¿ç”¨ `paired t-test` / `Mann-Whitney Uæ£€éªŒ` åˆ†ææŒ‡æ ‡æ˜¾è‘—å·®å¼‚
2. **è´¡çŒ®åˆ†æé‡åŒ–**ï¼š
   - è®¡ç®—æ¯ä¸ªstateåˆ†é‡ç¼ºå¤±åå¯¹ä¸»è¦æŒ‡æ ‡çš„å¹³å‡é€€åŒ–é‡ï¼ˆDelta Gap, Delta Speedï¼‰
3. **ä¸»æˆåˆ†åˆ†æ**ï¼ˆPCAï¼‰ï¼š
   - å°†æ‰€æœ‰æŒ‡æ ‡åˆå¹¶ï¼Œå¯¹æ¯”fullä¸ablationåçš„åˆ†å¸ƒå·®å¼‚
4. **äº¤å‰æ³›åŒ–çŸ©é˜µ**ï¼š
   - è¡Œä¸ºè®­ç»ƒçŠ¶æ€è¡¨ç¤ºï¼Œåˆ—ä¸ºæµ‹è¯•çŠ¶æ€è¡¨ç¤ºï¼Œè®°å½•æ¯ç»„ç»„åˆçš„testæˆåŠŸç‡ï¼ˆåˆ†æè·¨çŠ¶æ€è¿ç§»æ€§èƒ½ï¼‰