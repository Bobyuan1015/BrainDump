# 1. DeepSeek中多头潜在注意力MLA

**普通多头注意力（Multi-Head Attention, MHA）**

普通多头注意力是Transformer模型的核心组成部分，它通过将注意力机制分成多个“头”来捕捉不同的信息。

- **原理：**

  ​					head =![image-20250530164012844](/Users/fangyuan/Documents/yf/workplace/git_code/BrainDump/llms/assets/image-20250530164012844.png)

  ![image-20250530163955295](/Users/fangyuan/Documents/yf/workplace/git_code/BrainDump/llms/assets/image-20250530163955295.png)

  

- **好处：** 允许模型在不同的表示子空间中并行地关注来自不同位置的不同信息，从而捕捉到更丰富的语义关系。

**DeepSeek中的多头潜在注意力（Multi-Head Latent Attention, MLA）**

其核心思想是引入“潜在（latent）”表示

- **改进：**
  - **引入潜在向量：** MLA在计算注意力时引入了额外的潜在向量（latent vectors）。这些潜在向量可以看作是对整个输入序列的某种全局摘要或特征表示。
  - **潜在向量的Q、K、V：** 在MLA中，Q、K、V的生成可能不再仅仅依赖于原始输入序列，而是结合了这些潜在向量。例如，Q可能来自当前的词，K和V可能来自整个序列的潜在表示，或者是潜在表示与原始序列的组合。
  - **目的：** MLA旨在通过潜在向量来**减少计算复杂度**，特别是当序列长度很长时。通过将注意力计算的焦点转移到维度更低的潜在空间，可以在保持表达能力的同时降低计算开销。
  - **举例（概念性）：** 假设我们有一个很长的文档。普通注意力可能需要计算每个词对之间（$N \times N$）的注意力分数。而MLA可能会先从整个文档中提取少量几个“潜在主题向量”，然后每个词只与这些主题向量计算注意力，从而将复杂度从$O(N^2)$降低到$O(N \times L)$，其中$L$是潜在向量的数量，$L \ll N$。

其核心思想都是利用某种低维的、概括性的潜在表示来优化注意力计算。

**普通注意力-计算复杂度：**

- $Q K^T$: $(N \times d_k) \times (d_k \times N) \rightarrow N \times N$ 矩阵乘法，复杂度为 $O(N^2 d_k)$。

- $\text{softmax}$ 和 $V$ 的加权求和：$O(N^2 d_v)$。

- 总复杂度为 $O(N^2 D)$，其中 $D$ 是隐层维度（可以近似为 $d_k$ 或 $d_v$）

 


虽然DeepSeek的具体MLA实现细节可能不完全公开，但其背后原理通常基于以下几种策略之一或组合：

  1. **稀疏注意力（Sparse Attention）**：不计算所有位置的注意力，只计算部分位置的注意力。
  2. **线性注意力（Linear Attention）**：通过改变Softmax的计算方式，使得注意力计算复杂度降为线性。
  3. **基于聚类或潜在向量的注意力**：这是MLA更直接的体现，即引入少量的潜在向量作为信息聚合点。

  我们以第三种，即引入固定数量的潜在向量 $L$（通常 $L \ll N$）为例来解释MLA的数学原理。

  假设我们有 $L$ 个可学习的潜在向量 $P \in \mathbb{R}^{L \times D}$。MLA通常会涉及以下几个步骤：

  - **步骤 1：从输入序列到潜在向量的注意力（Q=Latent, K=Input, V=Input）**
    - 潜在向量 $P$ 作为查询（Query）。
    - 原始输入 $X$ 作为键（Key）和值（Value）。



