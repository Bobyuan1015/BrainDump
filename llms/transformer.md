![image-20250530104341358](./assets/image-20250530104341358.png)

# 1. 位置编码

传统 RNN，天然具备顺序

![image-20250530104721373](./assets/image-20250530104721373.png)

transformer 通过 multhead attention 实现，但是丢失了顺序，所以要加入位置编码

![image-20250530104852684](./assets/image-20250530104852684.png)

![image-20250530104959178](./assets/image-20250530104959178.png)

在 Transformer 结构中，**位置编码（Positional Encoding）** 是一种用于为输入序列中的每个位置（如单词在句子中的顺序）添加位置信息的机制。由于 Transformer 的自注意力机制本身是**排列不变（permutation-invariant）**的（即不关心输入顺序），位置编码帮助模型理解序列中元素的相对或绝对位置。

---

### **位置编码的作用**

1. **补充位置信息**：让模型知道单词的顺序（例如“猫追狗”和“狗追猫”意义不同）。
2. **可泛化到长序列**：即使序列长度超过训练时的最大长度，位置编码也能通过数学规律推断位置。

---

### **位置编码的实现方式**

在原始 Transformer 论文中，位置编码通过正弦和余弦函数生成，公式如下：

![image-20250530105721413](./assets/image-20250530105721413.png)
显示 markdown 预览效果
其中：

- `pos`：单词在序列中的位置（从 0 开始）。
- `i`：维度索引（位置编码向量的第`i`维）。
- `d_model`：模型的隐藏层维度（例如 512）。

这种设计使得位置编码能够：

- 对不同位置生成唯一编码。
- 通过三角函数性质，让模型学习相对位置（如“位置 5”和“位置 6”的编码关系与“位置 10”和“位置 11”类似）。

### **具体例子**

假设我们有一个句子："I love cats"，其词向量维度为`d_model=4`（实际中通常为 512）。我们为每个单词添加位置编码：

1. **输入序列**：["I", "love", "cats"]
   位置：`pos=0`, `pos=1`, `pos=2`
2. **生成位置编码**（简化版，假设`d_model=4`）：
   - 对于`pos=0`（单词"I"）：
     - 第 0 维（`i=0`）：`sin(0 / (10000^(0/4))) = sin(0) = 0`
     - 第 1 维（`i=1`）：`cos(0 / (10000^(2/4))) = cos(0) = 1`
     - 第 2 维（`i=2`）：`sin(0 / (10000^(4/4))) = sin(0) = 0`
     - 第 3 维（`i=3`）：`cos(0 / (10000^(6/4))) ≈ cos(0) = 1`
     - 编码结果：`[0, 1, 0, 1]`
   - 类似地，`pos=1`（"love"）和`pos=2`（"cats"）的编码可能是`[0.84, 0.54, 0.01, 1]`和`[0.91, -0.42, 0.02, 0.99]`（实际值由计算得出）。
3. **最终输入**：词向量 + 位置编码
   - "I"的词向量假设是`[0.5, 0.1, -0.2, 0.3]`，加上位置编码后：
     `[0.5+0, 0.1+1, -0.2+0, 0.3+1] = [0.5, 1.1, -0.2, 1.3]`

### **为什么不用简单的位置序号？**

- 直接使用`0,1,2,...`会导致数值过大，干扰词向量（尤其是长序列）。
- 三角函数编码可以扩展到任意长度的序列，并保留相对位置关系。

## 1.1 **为什么使用 sin 与 cos 交替?**

![image-20250530121859066](./assets/image-20250530121859066.png)

### **多尺度感知 —— 多频率组合帮助模型理解长短依赖**

通过使用不同频率的 sin 和 cos，在编码中混合了：

- 高频（变化快）：表达**局部、短距离**的位置信息
- 低频（变化慢）：表达**全局、长距离**的位置信息

这样可以让 Transformer 同时感知词与词之间的短依赖和长依赖关系。

![image-20250530122005062](./assets/image-20250530122005062.png)

对任意两个位置 pos1 和 pos2，你会发现使用交替的 sin 和 cos 编码后，它们之间的**点积或差值**具有可预测的形式：![image-20250530122617170](./assets/image-20250530122617170.png)

![image-20250530162928454](./assets/image-20250530162928454.png)

这种关系意味着 Transformer 可以**轻易地计算两个位置之间的相对距离**，这对理解“谁离谁更近”很重要。

![image-20250530162600151](./assets/image-20250530162600151.png)

# 2.注意力机制

![image-20250530163507190](./assets/image-20250530163507190.png)

**head =** ![image-20250530164012844](./assets/image-20250530164012844.png)

![image-20250530163955295](./assets/image-20250530163955295.png)

## 2.1 最早的注意力机制是什么？当时处于什么目的提出来的？

最早的注意力机制可以追溯到 2014-2015 年，其开创性工作主要集中在**神经机器翻译（Neural Machine Translation, NMT）**领域。

- **代表性工作：**

  - **Bahdanau, Cho, Bengio (2014):** "Neural Machine Translation by Jointly Learning to Align and Translate"
  - **Luong, Pham, Manning (2015):** "Effective Approaches to Attention-based Neural Machine Translation"

- **当时处于什么目的提出来的？**

  在注意力机制出现之前，主流的 NMT 模型是基于编码器-解码器（Encoder-Decoder）架构的循环神经网络（RNN），通常是长短期记忆网络（LSTM）或门控循环单元（GRU）。这些模型面临的主要挑战是：

  1. **信息瓶颈（Information Bottleneck）：** 编码器需要将整个源语句压缩成一个固定长度的“上下文向量”（context vector）。当源语句很长时，这个固定长度的向量很难完整地编码所有必要的信息，导致长句翻译质量下降。模型往往会“忘记”源语句开头的信息。
     - **问题：** 类似于你把一本厚厚的书的内容全部塞进一个小小的 U 盘，很容易丢失信息。
  2. **长期依赖问题：** 传统 RNN 在处理长序列时，梯度可能会消失或爆炸，导致模型难以学习到相距较远词之间的依赖关系。

  **注意力机制的提出正是为了解决这两个核心问题。**

  - **解决信息瓶颈：** 注意力机制允许解码器在生成目标序列的每个词时，不必仅仅依赖于一个单一的、固定长度的上下文向量。相反，它能够动态地“关注”源语句中与当前目标词最相关的部分。
    - **类比：** 想象一个人工翻译者，在翻译句子时，他不会一次性记住整个句子，而是在翻译当前词时，会回过头去看源句中哪些词是相关的。注意力机制就是模拟了这种“选择性关注”的过程。
  - **解决长期依赖问题：** 通过允许解码器直接访问编码器中不同时间步的隐藏状态（作为 K 和 V），注意力机制为模型提供了一个跳跃式连接，从而避免了信息必须通过序列逐步传递的限制。这使得模型更容易捕捉到源序列中的长距离依赖。

## 2.2 普通的注意力机制 Attention Mechanism

注意力机制最早是在机器翻译领域提出的，目的是解决传统序列模型（如 RNN）在处理长序列时信息瓶颈和长期依赖的问题。它允许模型在处理序列的某个元素时，能够“关注”输入序列中所有或部分相关的元素。

- **原理：** 注意力机制可以被概括为：给定一个查询（query）向量，去查询一系列键（key）向量，并根据查询与键的相似度，对相应的值（value）向量进行加权求和，从而得到一个加权的上下文向量。

  ![image-20250530164552157](./assets/image-20250530164552157.png)

  **Q（Query）、K（Key）、V（Value）的原理**

  - **Q (Query) - 查询：** 代表当前需要处理的信息或要查询的目标。你可以把它理解为图书馆里你手上的“书名”或“搜索请求”。
  - **K (Key) - 键：** 代表可以被查询的信息。你可以把它理解为图书馆里每本书的“索引”或“关键词”。
  - **V (Value) - 值：** 代表与键相关联的实际信息内容。你可以把它理解为图书馆里每本书的“实际内容”。

  **为什么 Q 与 K 直接计算出分数了，为什么还要 V？**

  Q 和 K 的目的是计算“注意力分数”，也就是确定每个位置的“重要性”或“相关性”。但是，这些分数本身并不包含实际的信息内容。V 才是真正的信息载体。

  所以，Q 和 K 用于确定“关注哪里”，而 V 提供“关注了什么”。缺少 V，注意力机制就无法提供实际的信息输出。

---

## 2.3 Transformer 中 Q、K、V 是否一样?

**Transformer 中 Q、K、V 是否一样？**

在 Transformer 模型中，Q、K、V 在概念上是不同的，但在实现上，它们最初都来自于相同的输入序列。

- **编码器（Encoder）中的自注意力：**
  - 在编码器的每个自注意力层中，Q、K、V 都来自于同一个输入序列（例如，某个输入句子的词嵌入）。这意味着每个词都会查询所有其他词，从而捕捉整个句子内部的依赖关系。在这种情况下，尽管它们来自相同的输入，但 Q、K、V 会通过不同的线性变换矩阵$W_Q, W_K, W_V$投影到不同的表示空间。$$ Q=XW_Q \quad K = XW_K \quad V=XW_V$$ 其中$X$是输入序列的表示。
- **解码器（Decoder）中的自注意力：**
  - 与编码器类似，解码器中的自注意力也从其自身的输入序列（已生成的词）生成 Q、K、V。
- **解码器中的编码器-解码器注意力（Cross-Attention）：**
  - 这是 Transformer 中 Q、K、V 来源不同的地方。
    - **Q (Query)** 来自于解码器的前一个输出（即当前要生成的词的表示）。
    - **K (Key)** 和 **V (Value)** 来自于编码器的最终输出。
  - 这种设计使得**decoder**在生成目标序列的每个词时，能够“关注”到输入序列（源语言）中所有相关的词。 $$ Q=YW*Q \quad K=X*{enc}W*K \quad V= X*{enc} W $$其中$Y$是解码器当前部分的输入，$X\_{enc}$是编码器的输出。

## 2.4 自注意力（Self-Attention）是什么？

**自注意力（Self-Attention）是什么？**

自注意力是注意力机制的一种特殊形式，其中 Q、K、V 都来源于同一个序列。换句话说，模型在处理序列中的一个元素时，能够关注到序列中所有其他元素（包括它自己）。

- **原理：** 对于一个输入序列$X = (x_1, x_2, \dots, x_n)$，自注意力为序列中的每一个元素$x_i$计算一个输出表示$y_i$。在计算$y_i$时，它会综合考虑$x_i$与序列中所有其他元素$x_j$之间的关系。 具体地，对于序列中的每个位置$i$：

  1. $x_i$作为查询 Q。
  2. 序列中所有的$x_j$作为键 K 和值 V。
  3. 计算$x_i$与所有$x_j$的相似度，得到权重。
  4. 用这些权重对所有$x_j$的值进行加权求和，得到$y_i$。

  数学公式与普通注意力相同，只是 Q、K、V 都来自于相同的输入序列经过线性变换。

  ![公式](./assets/image-20250530165957966.png)

- **好处：**

  1. **捕捉长距离依赖：** 传统 RNNs 需要通过序列逐步传递信息，可能存在长期依赖问题。自注意力机制允许任何两个位置之间的信息直接交互，无论它们在序列中的距离有多远。这使得模型能够更好地捕捉句子中的长距离依赖关系。
     - **例子：** 在句子“The animal didn't cross the street because it was too tired.”中，"it"指向"animal"。自注意力可以直接将"it"和"animal"关联起来，而无需通过中间词逐层传递信息。
  2. **并行计算：** 由于每个词的注意力计算可以独立进行，自注意力机制可以高度并行化，这极大地加速了模型的训练过程，尤其是在 GPU 等并行计算硬件上。这是 Transformer 模型能够处理长序列的关键之一。
  3. **捕获全局信息：** 每个词的表示都包含了整个序列的信息，因为它与其他所有词都进行了交互。这使得模型能够生成更丰富、更具上下文意义的表示。
  4. **对语序不敏感（结合位置编码）：** 纯粹的自注意力机制是置换不变的，这意味着如果打乱输入序列的顺序，其输出也会以同样的方式打乱。为了解决这个问题，Transformer 引入了“位置编码”（Positional Encoding），将序列中词的位置信息融入到词的嵌入中，从而使模型能够感知词的顺序。

- **为什么可以达到这样的效果？**

  - **直接连接：** 自注意力打破了传统序列模型（如 RNN）的链式结构，使得序列中的任意两个位置之间都存在一条直接的路径。这种“全连接”的特性使得信息传播更加高效。
  - **权重学习：** 注意力权重是模型通过训练数据学习到的，这意味着模型可以根据任务和上下文的需要，动态地分配不同位置的重要性。
  - **矩阵运算：** 所有的 QKV 计算、点积和加权求和都可以表示为高效的矩阵乘法，这非常适合现代深度学习框架和硬件的优化。

# 3.LayerNorm

| ![image-20250530171106754](./assets/image-20250530171106754.png) | ![image-20250530171139841](./assets/image-20250530171139841.png) |
| ----------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------- |

上图是 BatchNorm，按照维度去进行 norm，这在 NLP 中有两个问题：

- 1.句子长度不一样
- 2.例子中 两句话中 **我** 与 **今** 表示的特征肯定不一样，所以不能做 norm。

## 3.1. **核心思想对比**

- **BatchNorm**：对同一特征维度（channel）跨样本（batch）进行归一化，依赖 batch 统计量（均值/方差）。

  - 公式：对每个特征维度 _j_，计算 batch 内所有样本的均值和方差：

  ![image-20250530172116853](./assets/image-20250530172116853.png)

- **LayerNorm**：对单个样本的所有特征维度进行归一化，不依赖 batch。

  - 公式：对每个样本 _i_，计算其所有特征的均值和方差：

  ![image-20250530172129411](./assets/image-20250530172129411.png)

  - 在 RNN/Transformer 中，序列长度可变，BatchNorm 难以对齐不同长度的样本。LayerNorm 对每个时间步独立归一化，更适合序列模型。
