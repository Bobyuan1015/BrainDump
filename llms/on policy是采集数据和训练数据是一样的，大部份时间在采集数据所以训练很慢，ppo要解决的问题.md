on policy是采集数据和训练数据是一样的，大部份时间在采集数据所以训练很慢，ppo要解决的问题

Return大于0，则增加这个trajectory上的所有state下当前action的概率；反之，减少概率

ppo改进空间： 1.是否增大s下增大a的概率，应该看做了这个动作之后到游戏结束的reward，而不应该是整个累计的reward；因为这个action只会影响之后的reward，不会影响之前的。

2.action只影响接下来的几步，后面只会衰减。



采样越多， 方差越大 偏差越小

采样步数越小，越多的





