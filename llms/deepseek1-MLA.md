你是一个ai科学家，请你回答如下问题： deepseek中多头潜在注意力（MLA）是什么 和普通的多头注意力有什么区别；普通的注意力是什么，其中q K V的原理是什么，q与k直接计算出分数了，为社么还要v；transformer中q k v是一样么， 和自注意力有什么区别，自注意力是什么，有什么好处。有分别是什么样的原理，这样做的好处有什么，为什么可以达到这样的效果；最早的注意力机制是什么，是当时处于什么目的提出来的？ 

回答要求： 

- 回答要求带有数学公式、数学原理详细说明，最好带上例子说明。  
- 回答要求使用中文

- 